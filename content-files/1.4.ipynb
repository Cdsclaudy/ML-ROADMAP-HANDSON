{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4. TRAINING, VALIDATION AND TEST DATA\n",
    "## INTRODUCTION\n",
    "Training, validation and test data sets are three types of data sets that are commonly used in machine learning to create and evaluate predictive models. They have different purposes and roles in the machine learning process.\n",
    "\n",
    "## TRAINING DATA SET\n",
    "A **training data set is a data set of examples used to fit the parameters of the model**, such as the weights of the connections in an artificial neural network. The model is trained on the training data set using a supervised learning method, such as gradient descent. The training data set usually consists of pairs of an input vector and the corresponding output vector, also known as the target or label. The goal of the training process is to minimize the error between the model’s predictions and the targets on the training data set.\n",
    "\n",
    "## VALIDATION DATA SET\n",
    "A **validation data set is a data set of examples used to tune the hyperparameters of the model, such as the learning rate or the number of hidden layers**. The validation data set is separate from the training data set and is not used to fit the parameters of the model. Instead, it is used to evaluate how well the model generalizes to new data that it has not seen before. The validation data set provides a measure of the model’s performance and helps to select the best model among different candidates.\n",
    "\n",
    "## TEST DATA SET\n",
    "A **test data set is a data set of examples used to test the final performance of the model after it has been trained and validated**. The test data set is also separate from the training and validation data sets and is only used once at the end of the machine learning process. The test data set should reflect the real-world data that the model will encounter in practice. The test data set provides an unbiased estimate of the model’s accuracy and error rate on unseen data.\n",
    "\n",
    "## CONCLUSION\n",
    "Training, validation and test data sets are essential for building and evaluating machine learning models. They have different roles and functions in the machine learning process:\n",
    "- **Training data set**: used to fit the parameters of the model.\n",
    "- **Validation data set**: used to tune the hyperparameters of the model and select the best model.\n",
    "- **Test data set**: used to test the final performance of the model on unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HANDS ON FOR TRAIN, VALIDATION AND TEST DATA SPLITTING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. USING SkLearn's `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset and get X and Y data\n",
    "iris = load_iris()\n",
    "train = pd.DataFrame(iris.data)\n",
    "test = pd.DataFrame(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (90, 4)\n",
      "X_test shape: (30, 4)\n",
      "y_train shape: (90, 1)\n",
      "y_test shape: (30, 1)\n",
      "X_val shape: (90, 1)\n",
      "y val shape: (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# set aside 20% of train and test data for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, test,\n",
    "    test_size=0.2, shuffle = True, random_state = 8)\n",
    "\n",
    "# Use the same function above for the validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "    test_size=0.25, random_state= 8) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "print(\"X_val shape: {}\".format(y_train.shape))\n",
    "print(\"y val shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. USING Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/'\n",
    "'master/iris.csv') \n",
    "\n",
    "# one line split \n",
    "train, validation, test = np.split(iris.sample(frac=1), [int(.6*len(iris)),\n",
    "int(.8*len(iris))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (90, 4)\n",
      "X_test shape: (30, 4)\n",
      "y_train shape: (90,)\n",
      "y_test shape: (30,)\n",
      "X_val shape: (30, 4)\n",
      "y_val shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "# Assign the train split\n",
    "X_train = train[[train.columns[i] for i in range(train.shape[1]-1) ]]\n",
    "y_train = train[train.columns[-1]]\n",
    "# Assign the test split\n",
    "X_test = test[[test.columns[i] for i in range(train.shape[1]-1) ]]\n",
    "y_test = test[test.columns[-1]]\n",
    "# Assign the validation split\n",
    "X_val = validation[[validation.columns[i] for i in \n",
    "                    range(validation.shape[1]-1) ]]\n",
    "y_val = validation[validation.columns[-1]]\n",
    "\n",
    "# Print the sets data shapes\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"y_val shape: {}\".format(y_val.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "1. https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets\n",
    "2. https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "3. https://www.applause.com/blog/training-data-validation-data-vs-test-data\n",
    "4. https://machinelearningmastery.com/training-and-validation-data-in-pytorch/\n",
    "5. https://vitalflux.com/machine-learning-training-validation-test-data-set/\n",
    "6. https://towardsdatascience.com/how-to-split-data-into-three-sets-train-validation-and-test-and-why-e50d22d3e54c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdllab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
