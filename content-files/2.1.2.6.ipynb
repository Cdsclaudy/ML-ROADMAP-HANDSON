{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2.6. SUPPORT VECTOR MACHINE(SVM)\n",
    "\n",
    "## DEFINITION\n",
    "**SVM (Support Vector Machine)** is a powerful and popular machine learning algorithm for classification problems. It is **based on the idea of finding the optimal hyperplane that separates the data points into different classes with the maximum margin**.\n",
    "\n",
    "## STEPS\n",
    "1. Given a set of labeled data points that belong to two classes, find the hyperplane that maximizes the margin between the classes. The margin is defined as the distance between the hyperplane and the closest data points from each class, which are called support vectors. The hyperplane can be represented by a linear equation of the form w^T x + b = 0, where w is the normal vector and b is the bias term.\n",
    "2. To find the optimal hyperplane, we need to solve an optimization problem that minimizes the norm of w subject to some constraints that ensure that the data points are correctly classified. This can be done using a technique called Lagrange multipliers, which transforms the problem into a dual form that involves only the dot products between the data points.\n",
    "3. To make predictions for a new data point x, we need to evaluate the sign of w^T x + b. If it is positive, then x belongs to the positive class; if it is negative, then x belongs to the negative class; if it is zero, then x lies on the hyperplane.\n",
    "\n",
    "## ADVANTAGES\n",
    "- It can achieve high accuracy and generalization performance with a small number of support vectors, which reduces the computational cost and memory requirement.\n",
    "- It can handle both linear and nonlinear classification problems by using different kernels, which are functions that map the data points into a higher-dimensional feature space where they become linearly separable.\n",
    "- It can deal with imbalanced data sets by using different penalty parameters for different classes, which allows us to control the trade-off between precision and recall.\n",
    "\n",
    "## DISADVANTAGES\n",
    "- It can be sensitive to noise and outliers, which can affect the position of the hyperplane and reduce the margin.\n",
    "- It can be difficult to choose the best kernel and its parameters, which can have a significant impact on the performance and complexity of the model.\n",
    "- It does not provide probability estimates for the predictions, which can be useful for some applications.\n",
    "\n",
    "## KERNEL CHOICE\n",
    "Choosing the best kernel and its parameters is not trivial and depends on several factors, such as:\n",
    "\n",
    "- **The characteristics and distribution of the data set**. A linear kernel may work well for simple or linearly separable data sets, while a nonlinear kernel may be needed for complex or nonlinearly separable data sets.\n",
    "- **The trade-off between bias and variance**. A more complex kernel may lead to low bias but high variance, meaning that it can capture the nonlinear patterns well but may overfit the noise. A simpler kernel may lead to high bias but low variance, meaning that it can avoid overfitting but may miss some important details.\n",
    "- **The computational cost and efficiency**. A more complex kernel may require more computation time and memory space than a simpler kernel.\n",
    "\n",
    "## OPTIMALITY\n",
    "One way to find the optimal kernel and its parameters is to use cross-validation, which involves splitting the data into training and validation sets, applying different kernels and parameters on the training set, and evaluating their performance on the validation set. The kernel and parameters that minimize the validation error can be chosen as the best ones.\n",
    "\n",
    "## CONCLUSION\n",
    "In conclusion, SVM is a powerful and popular machine learning algorithm for classification problems that relies on finding the optimal hyperplane that separates the data points into different classes with the maximum margin. It has some advantages such as being accurate and flexible to different types of data, but also some disadvantages such as being sensitive to noise and outliers and requiring a good choice of kernel and parameters. Choosing the best kernel and parameters is crucial for achieving good results with SVM and can be done using cross-validation or other methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HANDS-ON: SVM Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "data['species'] = iris['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('species', axis=1)\n",
    "y = data['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SVM CLASSIFIER WITH LINEAR KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel='linear', C=1)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PREDICTION AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Measure the performance of the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "1. https://towardsdatascience.com/svm-support-vector-machine-for-classification-710a009f6873\n",
    "2. https://scikit-learn.org/stable/modules/svm.html\n",
    "3. https://link.springer.com/chapter/10.1007/978-1-4302-5990-9_3\n",
    "4. https://www.geeksforgeeks.org/introduction-to-support-vector-machines-svm/\n",
    "5. https://www.geeksforgeeks.org/support-vector-machine-algorithm/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdllab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
