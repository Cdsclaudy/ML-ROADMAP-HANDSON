{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2.7. GAUSSIAN MIXTURE MODELS\n",
    "## INTRODUCTION\n",
    "Gaussian Mixture Models (GMM) are a probabilistic model that assumes that the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. GMM can be used for both unsupervised and supervised learning but we'll be focusing on classification with supervised learning.\n",
    "\n",
    "## STEPS\n",
    "\n",
    "1. Given a set of labeled data points that belong to C classes, assume that each class is modeled by a mixture of K Gaussian components, where K is a predefined parameter that represents the number of components per class. Each Gaussian component has a mean vector, a covariance matrix, and a mixing weight that indicates the proportion of the component in the class. The total number of parameters to be estimated is C * (K * (D + 1) + (D * (D + 1)) / 2), where D is the dimensionality of the data.\n",
    "2. To estimate the parameters of GMM, we need to maximize the log-likelihood function that measures how well the model fits the data. This can be done using an iterative algorithm called expectation-maximization (EM), which alternates between two steps: expectation (E) step and maximization (M) step. In the E step, we compute the posterior probabilities of each data point belonging to each component and each class, using the current values of the parameters. In the M step, we update the parameters by using the posterior probabilities as weights for computing the sufficient statistics.\n",
    "3. To make predictions for a new data point x, we need to evaluate the posterior probability of x belonging to each class, using the estimated parameters of GMM. The class with the highest posterior probability is assigned to x.\n",
    "\n",
    "## ADVANTAGES\n",
    "\n",
    "- It can handle both linear and nonlinear classification problems by using different covariance structures for the Gaussian components, such as spherical, diagonal, tied or full covariance.\n",
    "- It can capture the multimodal nature of the data by using multiple components per class, which can represent different subgroups or clusters within each class.\n",
    "- It can provide soft assignments of data points to classes by using posterior probabilities instead of hard decisions.\n",
    "\n",
    "## DISADVANTAGES\n",
    "\n",
    "- It can be computationally expensive and slow, especially when dealing with large and high-dimensional data sets and many components per class.\n",
    "- It can be sensitive to noise and outliers, which can affect the estimation of the parameters and reduce the classification accuracy.\n",
    "- It can suffer from overfitting or underfitting, depending on the choice of K and the covariance structure. Overfitting occurs when K is too large or the covariance is too flexible, leading to overcomplex models that fit the noise. Underfitting occurs when K is too small or the covariance is too restrictive, leading to oversimplified models that miss some important details.\n",
    "\n",
    "## K-VALUE OPTIMALITY\n",
    "Choosing the best value of K and the covariance structure is not trivial and depends on several factors, such as:\n",
    "\n",
    "- **The characteristics and distribution of the data set**. A more complex or diverse data set may require a larger K and a more flexible covariance structure to capture its features, while a simpler or more homogeneous data set may require a smaller K and a more restrictive covariance structure to avoid overfitting.\n",
    "- **The trade-off between bias and variance**. A larger K and a more flexible covariance structure may lead to low bias but high variance, meaning that they can fit the data well but may be unstable and sensitive to noise. A smaller K and a more restrictive covariance structure may lead to high bias but low variance, meaning that they can avoid noise but may miss some important details.\n",
    "- **The computational cost and efficiency**. A larger K and a more flexible covariance structure may require more computation time and memory space than a smaller K and a more restrictive covariance structure.\n",
    "\n",
    "One way to find the optimal value of K and the covariance structure is to use cross-validation, which involves splitting the data into training and validation sets, applying different values of K and covariance structures on the training set, and evaluating their performance on the validation set. The value of K and the covariance structure that minimize the validation error can be chosen as the best ones.\n",
    "\n",
    "## CONCLUSION\n",
    "In conclusion, GMM is a probabilistic model for classification problems that assumes that the data points are generated from a mixture of Gaussian distributions with unknown parameters. It has some advantages such as being able to handle both linear and nonlinear classification problems and capture the multimodal nature of the data, but also some disadvantages such as being computationally expensive and sensitive to noise and overfitting. Choosing the best value of K and the covariance structure is crucial for achieving good results with GMM and can be done using cross-validation or other methods. GMM is a useful and flexible tool for classification problems that can be applied to various domains and applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HANDS-ON: GMM FOR CLASSIFICATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "data['species'] = iris['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('species', axis=1)\n",
    "y = data['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. GMM CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(n_components=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture(n_components=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianMixture(n_components=3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the GMM Classifier\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PREDICTIONS AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28888888888888886\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "y_pred = gmm.predict(X_test)\n",
    "\n",
    "# Measure the performance of the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "1. https://bing.com/search?q=Gaussian+Mixture+Models+for+Classification\n",
    "2. https://ieeexplore.ieee.org/document/8914215/\n",
    "3. https://scikit-learn.org/stable/modules/mixture.html\n",
    "4. https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd\n",
    "5. https://ieeexplore.ieee.org/document/8631558\n",
    "6. https://www.sciencedirect.com/science/article/pii/S016794730600510X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdllab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
